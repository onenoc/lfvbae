{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Problem: What is the problem your paper addresses? Avoid describing the solution here.\n",
    "Approximate bayesian computation uses simulators to help approximate the true posterior of a distribution when the likelihood is intractable.\n",
    "2. Importance: How would a solution to this problem change the world? Remember that it’s not obvious to everyone else how important this problem is.\n",
    "This allows bayesian inference to be done on a much wider class of models\n",
    "3. Challenge: Why is this problem hard? and/or What difficulty do other solutions face?\n",
    "Most traditional approaches use sampling methods, which are slow and thus do not handle expensive simulators well.\n",
    "4. New capability: What can we do now that we couldn’t do before? Quantify if possible.\n",
    "Our approach enables far fewer calls to the simulator\n",
    "5. Background: A sentence or two about other approaches; yours or others.\n",
    "Meeds and Welling use Gaussian Process and Hamiltonian Monte Carlo to speed up sampling methods, while Tran, Nott, and Kohn (2015) use variational inference by directly minimizing an approximation to the KL-divergence that introduces new variables.\n",
    "6. Insight: What did you discover? or How did you approach the problem differently?\n",
    "By using automatic differentiation, we can do variational inference by maximizing the lower bound without introducing new variables.\n",
    "7. Solution: Provide some specific detail about the solution.\n",
    "Our algorithm measures the standard deviation of the quantitative features in each dimension, then sorts the result, providing an optimal solution.\n",
    "8. Evidence: Summarize the evidence you have for your approach: A proof, an implementation, or quantiative results.\n",
    "Our algorithm achieves near-perfect performance on Bayesian linear regression, and substantially beats the performance of Trann, Nott, and Kohn on fitting an alpha-stable distribution.\n",
    "\n",
    "Approximate bayesian computation uses simulators to help approximate the true posterior of a distribution when the likelihood is intractable.  This allows bayesian inference to be done on a much wider class of models.  Most traditional ABC approaches use sampling methods, which are slow and thus do not handle expensive simulators well.  Our approach enables far fewer calls to the simulator.  Meeds and Welling use Gaussian Process and Hamiltonian Monte Carlo to speed up sampling methods, while Tran, Nott, and Kohn (2015) use variational inference by directly minimizing an approximation to the KL-divergence that introduces new variables.  By using automatic differentiation, we can do variational inference by maximizing the lower bound without introducing new variables.  Our algorithm achieves near-perfect performance on Bayesian linear regression, and substantially outperforms Trann, Nott, and Kohn when fitting an alpha-stable distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Questions and Ideas\n",
    "1.  Does the gradient estimator of KL have higher variance than the gradient estimator of lower bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:<br/>\n",
    "1.  Backround and Related Work<br/>\n",
    "    a.  What is approximate bayesian computation? <br/>\n",
    "    b.  Why is it important? <br/>\n",
    "    c.  Why is it hard? <br/>\n",
    "    d.  How have others tried to solve it? <br/>\n",
    "2.  The model<br/>\n",
    "    a.  decompose<br/>\n",
    "    b.  Lower bound<br/>\n",
    "    c.  reparametrization trick<br/>\n",
    "    d.  KL divergence analytic form<br/>\n",
    "    e.  single samples of $u$ and $\\nu$<br/>\n",
    "    f.  form of ABC kernel<br/>\n",
    "    g.  final objective function<br/>\n",
    "3.  Linear Regression<br/>\n",
    "    a.  The global optimum is when $q(\\theta)=p(\\theta|X)$<br/>\n",
    "        i.  when our recognition model has same parametric form as true posterior, parameters of global optimum are parameters of true posterior<br/>\n",
    "\n",
    "b.  Bayesian linear regression:<br/>\n",
    "4.  Wright Fisher<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Backround and Related Work<br/>\n",
    "    a.  What is approximate bayesian computation, and why is it important? <br/>\n",
    "\n",
    "\n",
    "\n",
    "    b.  Why is it important? <br/>\n",
    "        1.  However, for many problems in science, particularly in domains such as population genetics, ecology, and astronomy, the likelihoods are either intractable or extremely expensive to compute.\n",
    "    c.  Why is it hard? <br/>\n",
    "        1.  Two basic methods: rejection sampling and metropolis hastings\n",
    "        2.  Rejection sampling\n",
    "            a.  If proposal distribution much \"larger\" than true posterior, very slow\n",
    "            b.  does not work for high dimensional case\n",
    "        3.  Metropolis Hastings \n",
    "            a.  solves high dimensional issue\n",
    "            b.  long time to converge\n",
    "            c.  can spend long periods in low probability regions\n",
    "            d.  thus not good for expensive simulators\n",
    "    d.  How have others tried to solve it? <br/>\n",
    "        1.  Meeds and Welling (2014)\n",
    "        2.  Meeds, Leenders, and Welling (2015)\n",
    "        3.  Tran, Nott, and Kohn (2015)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.  Background and Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section Outline:\n",
    "1.  Why is Bayesian inference important for science?\n",
    "2.  What is Bayesian inference, and what are the problems?\n",
    "3.  How is intractable Bayesian inference done today?  What is the problem with that (rejection sampling ineffective for high dimensions, MCMC slow for expensive simulators)?\n",
    "4.  What are some solutions?\n",
    "    a.  sampling-based (sequential monte carlo, gaussian process)\n",
    "    b.  How did Tann, Nott, and Kohn do it?\n",
    "5.  How do we propose to do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter estimation and model selection are both important in a wide variety of scientific problems.  For parameter estimation, we may want to estimate evolutionary history based on genetic data, whether the market is in a boom or bust cycle, or what parameters were used to generate simulated data.  The latter is known as the inverse problem, and is an important problem in its own right.  For model selection, we have several models and we want to choose the model that best explains the observed data.  In many cases, having a distribution over parameter estimates is useful, as this gives information about the uncertainty of the estimate.  Thus, Bayesian methods are important to these classes of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian inference, one often wants to infer the posterior distribution $\\pi(\\theta|y)$ of some parameters or latent variables $\\theta$, given observations $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi(\\theta|y)=\\frac{\\pi(y|\\theta)\\pi(\\theta)}{\\int \\pi(y|\\theta)\\pi(\\theta)d\\theta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\pi(y|\\theta)$ is the likelihood, $\\pi(\\theta)$ is a prior on theta, and $\\int \\pi(y|\\theta)\\pi(\\theta)d\\theta=\\pi(y)$ is the normalizing constant.  Often, the integral involved in calculating the normalizing constant is intractable, and alternatives such as sampling-based or variational methods are used.  Both sampling-based and variational methods assume knowledge of the likelihood.  However, for many problems in science, particularly in domains such as population genetics, ecology, and astronomy, the likelihoods are either intractable or extremely expensive to compute.  Fortunately, these fields often have simulators that allow synthetic data $x$ to be generated according to the parameters $\\theta$, i.e. we can sample $x\\sim \\pi(x|\\theta)$.  These are then compared to the true data via an $\\epsilon$-kernel or ABC-kernel $K_{\\epsilon}(y,x)$, which is an approximation to $\\pi(y|x)$ and is a density that measures the disrepancy between x and y, where $\\epsilon$ is the bandwith, and serves to control how close $x$ and $y$ should be to have a high acceptance probability.  The likelihood, $\\pi(y|\\theta)$, is approximated by $\\pi_{\\epsilon}(y|\\theta)=\\int K_{\\epsilon}(y,x) \\pi(x|\\theta) \\: dx$, and this approximation is exact, i.e. $\\pi_{\\epsilon}(y|\\theta) = \\pi (y|\\theta)$ when $K_{\\epsilon=0}(y,x)=\\delta(y,x)$, where $\\delta$ is the dirac-delta function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.1 Sampling-based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most popular ABC algorithms are the ABC rejection algorithm and a modified Metropolis Hastings algorithm.  In the ABC rejection algorithm, we first sample $\\theta\\sim\\pi(\\theta)$.  Next, sample $x\\sim \\pi(x|\\theta)$.  Finally, accept $\\theta$ with probability proportional to $K_\\epsilon(y,x)$.  This approach has problems: if the bandwith $\\epsilon$ is too small or if the data is high dimensional, it will spend most of the time rejecting samples.  If $\\epsilon$ is too large, it will not give a good approximation and will be too heavily influenced by the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular alternative is to use a modified Metropolis-Hastings as follows.  At every time $t$, we propose a move from $\\theta$ to $\\theta'$ by drawing $\\theta'\\sim q(\\theta'|\\theta)$.  We also sample $x'\\sim \\pi(x'|\\theta)$.  Then we want to accept $\\theta'$ with probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha(\\theta'|\\theta) = \\min\\bigg(1, \\frac{\\pi_{\\epsilon}(y|\\theta') q(\\theta',\\theta)\\pi(\\theta')}{\\pi_{\\epsilon}(y|\\theta) q(\\theta, \\theta')\\pi(\\theta)}\\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scales well to high dimensions, but has the drawback that it requires the approximation $\\pi_{\\epsilon}(y|\\theta) \\approx \\frac{1}{L}\\sum_{l=1}^{L}K_{\\epsilon}(y,x_{l}^{\\theta})$, and thus requires $L$ calls to the simulator at each iteration.  For expensive simulators and with potentially slow mixing rates, this is impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, several techniques have been introduced to reduce the number of calls to the simualator.  Meeds and Welling developed sampling methods using Gaussian Processes (2014) and Hamiltonian Monte Carlo (2015), while Tran, Nott, and Kohn (2015) use variational inference, which in general converges much faster than sampling-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.2 Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Inference does approximate inference by minimizing the KL divergence between a recognition model, $q(\\theta)$, and the true posterior, $\\pi(\\theta|y)$.  The KL divergence is a non-symmetric measure of the disrepancy between two distributions.  Doing this, $q(\\theta)$ gives an approximation of $\\pi(\\theta|y)$.  This is often difficult to do directly, but we can decompose the log of the marginal probability into the KL divergence and a lower bound $\\mathcal{L}$ as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ln \\pi(y) = KL(q(\\theta) || \\pi(\\theta | y)) + \\mathcal {L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathcal{L} = \\int q(\\theta) \\ln \\frac{\\pi(y|\\theta)\\pi(\\theta)}{q(\\theta)} \\:d\\theta\\\\$.  Since the KL divergence may not be differentiable, variational inference usually involves maximizing the lower bound.  However, in the ABC case, doing so would require differentiating with respect to the approximate-likelihood, which would involve differentiating with respect to simulators, which may be very complex pieces of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tran, Nott, and Kohn (2015) take an alternative approach in order to avoid maximizing the lower bound.  They first introduce a scalar $z=\\log \\hat{p}_{N}(x|\\theta) - \\log p(y|\\theta)$, where  $\\hat{p}_{N}(x|\\theta)$ is an unbiased estimator of the true likelihood.  They then introduce the density $\\pi_{N}(\\theta,z)=\\pi(\\theta)e^{z}g_{N}(z|\\theta)$, where $\\pi(\\theta)$ is the true posterior that we want to estimate, and $g_{N}(z|\\theta)$ is the density of $z$.  They approximate $\\pi_{N}(\\theta,z)$ by $q_{\\lambda,N}(\\theta,z) = q_{\\lambda}(\\theta)g_{N}(z|\\theta)$, where $q_{\\lambda}(\\theta)$ is the variational distribution with parameters $\\lambda$ that approximates $\\pi(\\theta)$, the true posterior.  By introducing $z$, they work with $KL(q_{\\lambda,N}(\\theta,z) || \\pi(\\theta,z))$, which has a differentiable estimator, while $KL(q_{\\lambda}(\\theta)||\\pi(\\theta))$ may not.  They then show that under certain assumptions, the parameters $\\lambda$ that minimize  $KL(q_{\\lambda,N}(\\theta,z) || \\pi(\\theta,z))$ also minimize $KL(q_{\\lambda}(\\theta)||\\pi(\\theta))$.  For the case of approximation bayesian compuation, they use the same approximate likelihood above, $\\pi_{\\epsilon}(y|\\theta) \\approx \\frac{1}{L}\\sum_{l=1}^{L}K_{\\epsilon}(y,x_{l}^{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply variational inference to the case of ABC.  However, unlike, Tran, Nott, and Kohn, we instead maximize the lower bound by using automatic differentiation to differentiate with respect to simulators.  This allows us to avoid introducing $z$ and approximating $\\pi_{N}(\\theta,z)$ by $q_{\\lambda,N}(\\theta,z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I think we want to show<br/>\n",
    "1.  Is there some case where their assumptions don't hold?\n",
    "2.  That we have some better theoretical property than their theorem 01\n",
    "3.  Show that we beat them at fitting an alpha-stable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Approximate Bayesian Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In approximate bayesian computation (ABC), we want to estimate the posterior of a distribution in cases where the likelihood is intractable.  That is, we cannot compute $P(X|\\theta)$ cirectly, where $X$ is our observed data and $\\theta$ are some parameters, but we can generate synthetic data given some parameters via a simulator function $f(\\theta,u)$, which takes as input $\\theta$ and $u$, the latter being a randomness term for the simulator.  We want to use variational inference to approximate the posterior.  Variational inference is dependent on $P(X|\\theta)$, but we want to rewrite the objective function so that it is dependent on $P(X|f(\\theta,u))$.  In particular, we can rewrite the likelihood as follows.\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(X|\\theta) &=& \\int P(X|f(\\theta,u))P(f(\\theta,u)|\\theta)df\\\\\n",
    "&=& \\int P(X|f(\\theta,u))p(u)du\\\\\n",
    "&=& \\sum_{n=1}^{N}\\int P(x_{n}|f(\\theta,u))p(u)du\\\\\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then choose a distribution for $P(x_{n}|f(\\theta,u))= K_{\\epsilon}(x_{n},f(\\theta,u))$ as an acceptance kernel, to probabilistically accept or reject a datapoint.  For instance, if we choose $K_{\\epsilon}(x_{n},f(\\theta,u))=\\mathcal{N}(f,\\lambda^{-1})$, then\n",
    "$$\n",
    "P(X|\\theta)\\approx \\sum_{n=1}^{N}\\int \\mathcal{N}(x_{n}| f(\\theta,u),\\lambda^{-1})p(u)du\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  What is variational inference?\n",
    "2.  Variational ABC\n",
    "3.  High Variance Estimators and the Reparametrization Trick\n",
    "5.  Sampling and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower bound,  $\\mathcal{L}$, which we maximize, is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray}\n",
    "\\mathcal{L} &=& \\int q(\\theta) \\ln \\frac{\\pi(\\theta,y)}{q(\\theta)} \\:d\\theta\\\\\n",
    "&=& \\int q(\\theta)\\ln \\pi(y|\\theta) \\:d\\theta - \\int q(\\theta) \\ln \\frac{\\pi(\\theta)}{q(\\theta)} \\:d\\theta\\\\\n",
    "\\end{eqnarray}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our problem, we first note that $x$ is dependent on two parameters: $\\theta$, the parameter we want to estimate, and $u$, a randomness parameter for the simulator.  We can rewrite this in terms of $\\pi(y|x)$ as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray}\n",
    "\\mathcal{L} &=& \\int \\int q(\\theta) \\ln \\pi(y|x) p(u) \\:du \\:d\\theta-KL(q(\\theta) || \\pi(\\theta))\\\\\n",
    "&=& \\mathbb{E}_{\\theta\\sim q(\\theta),u\\sim\\mathcal{N}(0,1)}(\\ln p(y|x))-KL(q(\\theta) || \\pi(\\theta))\\\\\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##High Variance Estimators and Reparametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual monte carlo estimate of the gradient for the expectations is $\\frac{1}{L_{\\theta}L_{u}}\\sum_{l_{\\theta}=1}^{L_{\\theta}}\\sum_{l_{u}=1}^{L_{u}}ln\\:p(X|f(\\theta_{l_{\\theta}},u_{l_{u}}))$.  This has very high variance and will lead to poor optimization results when using gradient descent.  However, \n",
    "but using the reparametrization trick of Kingma and Welling (2013), if we use a normal distribution for the recognition model $q(\\theta)$, so that $\\theta$~$\\mathcal{N}(\\mu,\\sigma^{2})$, we can let $\\theta=\\mu+\\sigma\\cdot\\nu$, where $\\nu\\sim\\mathcal{N}(0,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sampling, Gradient Descent, and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}= \\mathbb{E}_{q(\\nu)}(\\mathbb{E}_{u}(\\ln p(X|f(\\mu+\\sigma\\cdot \\nu,u))))-KL(q(\\theta) || p(\\theta))\\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set our prior to $\\mathcal{N}(0,1)$, then the KL-divergence can be integrated analytically and has a closed form solution ($J$ is the number of dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}= \\mathbb{E}_{q(\\nu)}(\\mathbb{E}_{u}(\\ln p(X|f(\\mu+\\sigma\\cdot \\nu,u))))+\\frac{1}{2}\\sum_{j=1}^{J}(1+2\\log\\:\\sigma-\\mu^{2}-\\sigma^{2})\\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we can approximate the expectations with respect to $u$ and $q(\\nu)$ with single samples $u_{s}$ and $v_{l}$ for each expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray}\\mathcal{L}&\\approx& \\ln p(X|f(\\mu+\\sigma\\cdot \\nu_l,u_s))+\\frac{1}{2}\\sum_{j=1}^{J}(1+2\\log\\:\\sigma-\\mu^{2}-\\sigma^{2})\\\\\n",
    "&=& \\sum_{n=1}^{N}\\ln p(x_{n}|f(\\mu+\\sigma\\cdot \\nu_l,u_s))+\\frac{1}{2}\\sum_{j=1}^{J}(1+2\\log\\:\\sigma-\\mu^{2}-\\sigma^{2})\\\\\n",
    "\\end{eqnarray}\\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $\\ln p(X|f(\\mu+\\sigma\\cdot \\nu_l,u_s))$ represents our ABC rejection probability, so if we set that to $\\mathcal{N}(f,\\lambda)$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}\\approx -\\sum_{n=1}^{N}(\\log\\:2\\pi+\\log\\:\\lambda)-\\frac{1}{2}(y-f)^{T}(\\frac{1}{\\lambda^{2}})I(y-f)+\\frac{1}{2}\\sum_{j=1}^{J}(1+2\\log\\:\\sigma-\\mu^{2}-\\sigma^{2})\\\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $u$ gets multiplied by $R$ in the simulator, and that $R$~$log\\mathcal{N}(\\mu_{R},\\sigma_{R}^{2})$.  We thus need to estimate $\\mu,\\sigma,\\lambda,\\mu_{R}$, and $\\sigma_{R}$.  Since in order to take the relevant derivatives, we need to differentiate with respect to $f(\\theta,u)$, we use automatic differentiation on the entire lower bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Test Case: Bayesian Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global max of the lower bound is when the recognition model is equal to the true posterior, i.e. $q(\\theta)=p(\\theta|X)$.  To show this, first note that $\\log\\: p(X) \\geq \\mathcal{L}$.  Now, let $q(\\theta)=p(\\theta|X)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray} \n",
    "\\mathcal{L} &=& \\int q(\\theta) \\ln \\frac{p(\\theta,X)}{q(\\theta)} \\:d\\theta\\\\\n",
    "&=& \\int q(\\theta) \\ln \\frac{p(\\theta|X)p(X)}{p(\\theta|X)} \\:d\\theta\\\\\n",
    "&=& \\int q(\\theta) \\ln p(X) \\:d\\theta\\\\\n",
    "&=& \\ln p(X) \\int q(\\theta) \\:d\\theta\\\\\n",
    "&=& \\ln p(X)\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our recognition model $q_{\\phi_{r}}(\\theta)$ has a parametric form with parameters $\\phi_{r}$, and our true posterior has the same parametric form, with parameters $\\phi_{t}$, then the optimum will be when $\\phi_{r}=\\phi_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test case, we use bayesian linear regression, where the prior is standard normal.  We generate data points $X$~$U(a,b)$, use a fixed weight vector $w$, and let $$y=X\\cdot w+\\epsilon$$ where $\\epsilon$~$\\mathcal{N}(0,\\sigma_{e}^{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we use steepest descent to minimize the negative lower bound with respect to $\\mu$ and $\\sigma$.  We set $u=0$, and thus we should converge to the true posterior, as $\\mathbb{E}_{q(\\nu)}(\\mathbb{E}_{u}(\\ln p(X|f(\\mu+\\sigma\\cdot \\nu,u))))$ becomes $\\mathbb{E}_{q(\\theta)}(\\ln p(X|\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://rsif.royalsocietypublishing.org/content/6/31/187.short\n",
    "http://bioinformatics.oxfordjournals.org/content/24/23/2713.short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (lfvbae.py, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"lfvbae.py\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    x0n =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "execfile(\"test.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD9CAYAAAChtfywAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104b137d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cost(encoder,.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
